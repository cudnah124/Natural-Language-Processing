{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFD7DVZ-xKdT"
      },
      "source": [
        "# Homework Lab 2: Text Preprocessing with Vietnamese\n",
        "**Overview:** In this exercise, we will build a text preprocessing program for Vietnamese."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOAeiqdrxKdt"
      },
      "source": [
        "Import the necessary libraries. Note that we are using the underthesea library for Vietnamese tokenization. To install it, follow the instructions below. ([link](https://github.com/undertheseanlp/underthesea))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtLC2vLI-McH",
        "outputId": "1ab6ca2f-e72d-407e-de5f-0c3263987c53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: zipfile36 in /usr/local/lib/python3.12/dist-packages (0.1.3)\n",
            "Requirement already satisfied: underthesea in /usr/local/lib/python3.12/dist-packages (9.1.4)\n",
            "Requirement already satisfied: patool in /usr/local/lib/python3.12/dist-packages (4.0.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2026.1.4)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.12/dist-packages (from underthesea) (8.3.1)\n",
            "Requirement already satisfied: python-crfsuite>=0.9.6 in /usr/local/lib/python3.12/dist-packages (from underthesea) (0.9.12)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from underthesea) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from underthesea) (1.6.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from underthesea) (6.0.3)\n",
            "Requirement already satisfied: underthesea_core>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from underthesea) (1.0.7)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from underthesea) (0.36.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.1->underthesea) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.1->underthesea) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->underthesea) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->underthesea) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->underthesea) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->underthesea) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->underthesea) (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install requests tqdm pandas zipfile36 underthesea patool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "RrFQ_Ht_xKdu"
      },
      "outputs": [],
      "source": [
        "import os,glob\n",
        "import codecs\n",
        "import sys\n",
        "import re\n",
        "from underthesea import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hC27lBQZxKdw"
      },
      "source": [
        "## Question 1: Create a Corpus and Survey the Data\n",
        "\n",
        "The data in this section is partially extracted from the [VNTC](https://github.com/duyvuleo/VNTC) dataset. VNTC is a Vietnamese news dataset covering various topics. In this section, we will only process the science topic from VNTC. We will create a corpus from both the train and test directories. Complete the following program:\n",
        "\n",
        "- Write `sentences_list` to a file named `dataset_name.txt`, with each element as a document on a separate line.\n",
        "- Check how many documents are in the corpus.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "beyLjPtZ-McK",
        "outputId": "afdb0755-5b0e-43ca-eed1-f3aa86f44999"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n",
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO patool: Extracting VNTC_raw/VNTC-master/Data/10Topics/Ver1.1/Train_Full.rar ...\n",
            "INFO patool: running /usr/bin/unrar x -kb -or -- /content/VNTC_raw/VNTC-master/Data/10Topics/Ver1.1/Train_Full.rar\n",
            "INFO patool: ... VNTC_raw/VNTC-master/Data/10Topics/Ver1.1/Train_Full.rar extracted to `VNTC_khoahoc'.\n",
            "INFO patool: Extracting VNTC_raw/VNTC-master/Data/10Topics/Ver1.1/Test_Full.rar ...\n",
            "INFO patool: running /usr/bin/unrar x -kb -or -- /content/VNTC_raw/VNTC-master/Data/10Topics/Ver1.1/Test_Full.rar\n",
            "INFO patool: ... VNTC_raw/VNTC-master/Data/10Topics/Ver1.1/Test_Full.rar extracted to `VNTC_khoahoc'.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'VNTC_khoahoc'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import patoolib\n",
        "\n",
        "URL = \"https://github.com/duyvuleo/VNTC/archive/refs/heads/master.zip\"\n",
        "ZIP_PATH = \"VNTC_master.zip\"\n",
        "EXTRACT_DIR = \"VNTC_raw\"\n",
        "FINAL_DIR = \"VNTC_khoahoc\"\n",
        "\n",
        "print(\"Downloading dataset...\")\n",
        "response = requests.get(URL)\n",
        "with open(ZIP_PATH, \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "print(\"Extracting files...\")\n",
        "with zipfile.ZipFile(ZIP_PATH, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(EXTRACT_DIR)\n",
        "\n",
        "TRAIN_PATH = os.path.join(EXTRACT_DIR, \"VNTC-master\", \"Data\", \"10Topics\", \"Ver1.1\", \"Train_Full.rar\")\n",
        "TEST_PATH = os.path.join(EXTRACT_DIR, \"VNTC-master\", \"Data\", \"10Topics\", \"Ver1.1\", \"Test_Full.rar\")\n",
        "\n",
        "patoolib.extract_archive(TRAIN_PATH, outdir=FINAL_DIR)\n",
        "patoolib.extract_archive(TEST_PATH, outdir=FINAL_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "GyNKT8wAxKdx",
        "outputId": "49da761f-79ec-4487-8429-891ce94ab6f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train labels = test labels\n",
            "Number of documents =  23496\n"
          ]
        }
      ],
      "source": [
        "dataset_name = \"VNTC_khoahoc\"\n",
        "path = ['./VNTC_khoahoc/Train_Full/', './VNTC_khoahoc/Test_Full/']\n",
        "\n",
        "if os.listdir(path[0]) == os.listdir(path[1]):\n",
        "    folder_list = [os.listdir(path[0]), os.listdir(path[1])]\n",
        "    print(\"train labels = test labels\")\n",
        "else:\n",
        "    print(\"train labels differ from test labels\")\n",
        "\n",
        "doc_num = 0\n",
        "sentences_list = []\n",
        "meta_data_list = []\n",
        "for i in range(2):\n",
        "    for folder_name in folder_list[i]:\n",
        "        folder_path = path[i] + folder_name\n",
        "        #Just Khoa Hoc for testing\n",
        "        if folder_name == 'Khoa hoc':\n",
        "            for file_name in glob.glob(os.path.join(folder_path, '*.txt')):\n",
        "                # Read the file content into f\n",
        "                f = codecs.open(file_name, 'br')\n",
        "                # Convert the data to UTF-16 format for Vietnamese text\n",
        "                file_content = (f.read().decode(\"utf-16\")).replace(\"\\r\\n\", \" \")\n",
        "                sentences_list.append(file_content.strip())\n",
        "                f.close\n",
        "                # Count the number of documents\n",
        "                doc_num += 1\n",
        "\n",
        "#### YOUR CODE HERE ####\n",
        "with open(dataset_name + '.txt', 'w', encoding='utf-8') as f:\n",
        "    for sentence in sentences_list:\n",
        "        f.write(sentence + '\\n')\n",
        "\n",
        "print(\"Number of documents = \", doc_num)\n",
        "#### END YOUR CODE #####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DA3cT7h-McK"
      },
      "source": [
        "## Question 2: Write Preprocessing Functions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KXHcDpuxKd0"
      },
      "source": [
        "### Question 2.1: Write a Function to Clean Text\n",
        "Hint:\n",
        "- The text should only retain the following characters: aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0-9(),!?\\'\\\n",
        "- Then trim the whitespace in the input text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "k8hIglDXxKd0"
      },
      "outputs": [],
      "source": [
        "allowed_chars = r\"a-zA-Z0-9àÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆiIìÌỉỈĩĨíÍịỊoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰyYỳỲỷỶỹỸýÝỵỴ(),!?'\\\" \"\n",
        "clean_pattern = re.compile(f\"[^{allowed_chars}]\")\n",
        "def clean_str(string):\n",
        "    #### YOUR CODE HERE ####\n",
        "    if not string:\n",
        "        return \"\"\n",
        "\n",
        "    cleaned_string = clean_pattern.sub('', string)\n",
        "\n",
        "    cleaned_string = re.sub(r'\\s+', ' ', cleaned_string)\n",
        "\n",
        "    return cleaned_string.strip()\n",
        "    #### END YOUR CODE #####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KfXstqAxKd1"
      },
      "source": [
        "### Question 2.2: Write a Function to Convert Text to Lowercase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "KRwgVjxhxKd1"
      },
      "outputs": [],
      "source": [
        "# make all text lowercase\n",
        "def text_lowercase(string):\n",
        "    #### YOUR CODE HERE ####\n",
        "    if string is not None:\n",
        "      string = string.lower()\n",
        "    return string\n",
        "    #### END YOUR CODE #####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYM_GO_5xKd2"
      },
      "source": [
        "### Question 2.3: Tokenize Words\n",
        "Hint: Use the `word_tokenize()` function imported above with two parameters: `strings` and `format=\"text\"`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "pty34NwyxKd2"
      },
      "outputs": [],
      "source": [
        "def tokenize(strings):\n",
        "    #### YOUR CODE HERE ####\n",
        "    return word_tokenize(strings, format=\"text\")\n",
        "    #### END YOUR CODE #####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gQGmL4gxKd2"
      },
      "source": [
        "### Question 2.4: Remove Stop Words\n",
        "To remove stop words, we use a list of Vietnamese stop words stored in the file `./vietnamese-stopwords.txt`. Complete the following program:\n",
        "- Check each word in the text (`strings`). If a word is not in the stop words list, add it to `doc_words`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "aqStv2rPxKd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c548e50-8c38-4ba8-a160-77f9896b009b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading stopwords...\n"
          ]
        }
      ],
      "source": [
        "STOPWORDS_URL = \"https://raw.githubusercontent.com/cudnah124/Natural-Language-Processing/main/lab2/vietnamese-stopwords.txt\"\n",
        "STOPWORDS_PATH = \"vietnamese-stopwords.txt\"\n",
        "\n",
        "print(\"Downloading stopwords...\")\n",
        "response = requests.get(STOPWORDS_URL)\n",
        "\n",
        "with open(STOPWORDS_PATH, \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "with open('./vietnamese-stopwords.txt', 'r', encoding='utf-8') as f:\n",
        "    STOP_WORDS = set([line.strip().replace(' ', '_') for line in f])\n",
        "def remove_stopwords(strings):\n",
        "    #### YOUR CODE HERE ####\n",
        "    doc_words = [word for word in strings.split() if word not in STOP_WORDS]\n",
        "    return ' '.join(doc_words)\n",
        "    #### END YOUR CODE #####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUNOKigIxKd4"
      },
      "source": [
        "## Question 2.5: Build a Preprocessing Function\n",
        "Hint: Call the functions `clean_str`, `text_lowercase`, `tokenize`, and `remove_stopwords` in order, then return the result from the function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "_vd-el91xKd_"
      },
      "outputs": [],
      "source": [
        "def text_preprocessing(strings):\n",
        "    #### YOUR CODE HERE ####\n",
        "    strings = clean_str(strings)\n",
        "    strings = text_lowercase(strings)\n",
        "    strings = tokenize(strings)\n",
        "    strings = remove_stopwords(strings)\n",
        "    return strings\n",
        "    #### END YOUR CODE #####\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BGOqa1mxKeA"
      },
      "source": [
        "## Question 3: Perform Preprocessing\n",
        "Now, we will read the corpus from the file created in Question 1. After that, we will call the preprocessing function for each document in the corpus.\n",
        "\n",
        "Hint: Call the `text_preprocessing()` function with `doc_content` as the input parameter and save the result in the variable `temp1`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4ikJ_uf-McN",
        "outputId": "a008f7d7-ca9c-402e-f92f-74e31550293d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 1000 documents\n",
            "Processed 2000 documents\n",
            "Processed 3000 documents\n",
            "Processed 4000 documents\n",
            "Processed 5000 documents\n",
            "Processed 6000 documents\n",
            "Processed 7000 documents\n",
            "Processed 8000 documents\n",
            "Processed 9000 documents\n",
            "Processed 10000 documents\n",
            "Processed 11000 documents\n",
            "Processed 12000 documents\n",
            "Processed 13000 documents\n",
            "Processed 14000 documents\n",
            "Processed 15000 documents\n",
            "Processed 16000 documents\n",
            "Processed 17000 documents\n",
            "Processed 18000 documents\n",
            "Processed 19000 documents\n",
            "Processed 20000 documents\n",
            "Processed 21000 documents\n",
            "Processed 22000 documents\n",
            "Processed 23000 documents\n",
            "\n",
            "length of clean_docs =  23496\n",
            "clean_docs[0]:\n",
            "đôi giày thể_hiện meghan_cleary , tác_giả sách tương_hợp hoàn_hảo , \" phụ_tùng trang_phục , đôi giày tiết_lộ trạng_thái tinh_thần phụ_nữ \" ý_nghĩa đôi giày ưng_ý kiểu giày balê_đế phẳng giỏi ngoại_giao , , chăm_sóc , thường_xuyên xoa_dịu , dàn hòa bất_đồng bạn_bè_bạn óc sáng_tạo nghiêm_túc kiểu giày cao_gót nhọn phối_hợp quyến_rũ truyền_thống hiện_đại , đầy_đủ sức_mạnh phụ_nữ tự_tin giày_hở gót năng_nổ , xông_xáo , thường_xuyên thoăn_thoắt công_sở bữa tiệc hơi nghịch_ngợm một_chút , giày vải quyến_rũ điềm_đạm , người_yêu trò_chuyện thông_minh , quan_sát nhanh_nhẹn phát_triển ngừng hoạt_động liên_tục hiểu_biết âm_nhạc , điện_ảnh giày sống tình có_lý , sợ đương_đầu gia_đình công_sở bạn_bè yêu quý_vẻ bình_dị , hài_hước\n"
          ]
        }
      ],
      "source": [
        "#### YOUR CODE HERE ####\n",
        "clean_docs = []\n",
        "with open(dataset_name + '.txt', 'r', encoding='utf-8') as f:\n",
        "  for doc_content in f:\n",
        "    temp1 = text_preprocessing(doc_content)\n",
        "    clean_docs.append(temp1)\n",
        "    if len(clean_docs) % 1000 == 0:\n",
        "      print(f\"Processed {len(clean_docs)} documents\")\n",
        "\n",
        "\n",
        "\n",
        "#### END YOUR CODE #####\n",
        "print(\"\\nlength of clean_docs = \", len(clean_docs))\n",
        "print('clean_docs[0]:\\n' + clean_docs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFhai6BwxKeB"
      },
      "source": [
        "## Question 4: Save Preprocessed Data\n",
        "Hint: Save the preprocessed data to a file named `dataset_name + '.clean.txt'`, where each document is written on a separate line.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "xfHmSiRrxKeB"
      },
      "outputs": [],
      "source": [
        "#### YOUR CODE HERE ####\n",
        "with open(dataset_name + '.clean.txt', 'w', encoding='utf-8') as f:\n",
        "  for doc in clean_docs:\n",
        "    f.write(doc + '\\n')\n",
        "#### YOUR CODE HERE ####"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}